{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "no_space = re.compile(r\"\\s+\")\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = url_pattern.sub(\" \", text)\n",
    "    text = no_space.sub(\" \", text)\n",
    "    text = text.strip().lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/qdxtf8y15yg7kk7psxbx4l4h0000gn/T/ipykernel_57092/4156909105.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "train_ques, train_body, train_pids = [], [], []\n",
    "with open(f\"{data_dir}/qa_train.txt\", 'r', encoding = \"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        qa_pair = json.loads(line)\n",
    "        train_ques.append(clean_data(qa_pair['question']))\n",
    "        train_body.append(clean_data(qa_pair['body']))\n",
    "        train_pids.append(qa_pair['pids'])\n",
    "\n",
    "valid_ques, valid_body = [], []\n",
    "with open(f\"{data_dir}/qa_valid_wo_ans.txt\", 'r', encoding = \"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        qa_pair = json.loads(line)\n",
    "        valid_ques.append(clean_data(qa_pair['question']))\n",
    "        valid_body.append(clean_data(qa_pair['body']))\n",
    "\n",
    "print(len(train_body), len(valid_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{data_dir}/pid_to_title_abs.json\", encoding='utf-8')\n",
    "papers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pids, all_tabs = [], []\n",
    "for pid, paper in papers.items():\n",
    "    all_pids.append(pid)\n",
    "    abs = clean_data(paper['abstract'])\n",
    "    if paper['title'] is not None:\n",
    "        tit = clean_data(paper['title'])    \n",
    "        all_tabs.append(f\"{tit} [SEP] {abs}\")\n",
    "    else:\n",
    "        all_tabs.append(abs)\n",
    "\n",
    "label_pid_to_id = {t_id: i for i, t_id in enumerate(all_pids)}\n",
    "\n",
    "data, row, col = [], [], []\n",
    "for i, pids in enumerate(train_pids):\n",
    "    for pid in pids:\n",
    "        row.append(i)\n",
    "        col.append(label_pid_to_id[pid])\n",
    "        data.append(1)\n",
    "\n",
    "train_Q_A = sp.csr_matrix((data, (row, col)), shape = (len(train_ques), len(all_pids)))\n",
    "sp.save_npz(f\"{data_dir}/train_Q_A.npz\", train_Q_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_dir}/train_ques.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(train_ques))\n",
    "\n",
    "with open(f\"{data_dir}/train_body.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(train_body))\n",
    "\n",
    "with open(f\"{data_dir}/valid_ques.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(valid_ques))\n",
    "\n",
    "with open(f\"{data_dir}/valid_body.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(valid_body))\n",
    "\n",
    "with open(f\"{data_dir}/papers.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_tabs))\n",
    "\n",
    "with open(f\"{data_dir}/all_pids.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Pretrain Data\n",
    "\n",
    "pretrain_title, pretrain_abs = [], []\n",
    "for k, v in papers.items():\n",
    "    if v['title'] is None or v['abstract'] is None:\n",
    "        continue\n",
    "    clean_title = clean_data(v['title']).lower()\n",
    "    clean_abs = clean_data(v['abstract']).lower()\n",
    "    \n",
    "    if len(clean_title) < 5 or len(clean_abs) < 50:\n",
    "        continue \n",
    "    \n",
    "    pretrain_title.append(clean_title)\n",
    "    pretrain_abs.append(clean_abs)\n",
    "\n",
    "pretrain_title, indices = np.unique(pretrain_title, return_index = True)\n",
    "pretrain_title = list(pretrain_title)\n",
    "pretrain_unq_abs = []\n",
    "\n",
    "for idx in indices:\n",
    "    pretrain_unq_abs.append(pretrain_abs[idx])\n",
    "\n",
    "with open(f\"{data_dir}/pretrain_title.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(pretrain_title))\n",
    "\n",
    "with open(f\"{data_dir}/pretrain_abstract.raw.txt\", \"w\", encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(pretrain_unq_abs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
